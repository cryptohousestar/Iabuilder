# üéØ RATE LIMITING INTELIGENTE - Groq CLI

## üìä RESUMEN

Implementado sistema de rate limiting **inteligente por modelo** que:
- ‚úÖ Detecta autom√°ticamente l√≠mites seg√∫n el modelo usado
- ‚úÖ Se actualiza cuando cambias de modelo con `/model`
- ‚úÖ **Muy conservador** (70-80% de l√≠mites reales) para evitar cortes
- ‚úÖ Soporta modelos **free** y **paid** de Groq
- ‚úÖ Usa animaci√≥n de carga cuando espera

---

## üîß C√ìMO FUNCIONA

### 1. L√≠mites Autom√°ticos por Modelo

Cada modelo tiene l√≠mites espec√≠ficos:

**Ejemplo: llama-3.3-70b-versatile (FREE)**
- L√≠mite real Groq: 30 RPM, 12K TPM
- L√≠mite configurado: **20 RPM, 8K TPM** (70% del l√≠mite real)
- **Por qu√©**: Para evitar cortes y confusi√≥n del modelo

**Ejemplo: llama-3.1-8b-instant (FREE)**
- L√≠mite real Groq: 30 RPM, 6K TPM
- L√≠mite configurado: **20 RPM, 4K TPM**

**Ejemplo: llama-3.3-70b-versatile (PAID)**
- L√≠mite real Groq: 1K RPM, 300K TPM
- L√≠mite configurado: **800 RPM, 240K TPM** (80% del l√≠mite real)

### 2. Cambio Autom√°tico de L√≠mites

Cuando usas `/model`:
```bash
> /model llama-3.1-8b-instant
```

El sistema autom√°ticamente:
1. Cambia el modelo
2. Actualiza los l√≠mites de rate limiting
3. Muestra informaci√≥n: `üìä Rate limits updated for llama-3.1-8b-instant: TPM: 4,000 | RPM: 20`

### 3. Prevenci√≥n de Cortes

El sistema verifica **DOS l√≠mites** antes de cada request:

1. **TPM (Tokens por Minuto)**
   - Cuenta cu√°ntos tokens usaste en el √∫ltimo minuto
   - Si est√°s cerca del l√≠mite, espera

2. **RPM (Requests por Minuto)**
   - Cuenta cu√°ntas requests hiciste en el √∫ltimo minuto
   - Si est√°s cerca del l√≠mite, espera

Si alguno se excede ‚Üí **Espera autom√°tica** con animaci√≥n de carga

---

## üìö MODELOS SOPORTADOS

### FREE TIER (Configurados Conservadoramente)

| Modelo | RPM | TPM | Notas |
|--------|-----|-----|-------|
| llama-3.3-70b-versatile | 20 | 8K | Modelo principal recomendado |
| llama-3.1-8b-instant | 20 | 4K | M√°s r√°pido, menor calidad |
| groq/compound | 20 | 50K | **L√≠mite TPM muy alto** |
| groq/compound-mini | 20 | 50K | Versi√≥n mini |
| qwen/qwen3-32b | 40 | 4K | **RPM m√°s alto** |
| moonshotai/kimi-k2-instruct | 40 | 7K | Modelo Kimi |
| openai/gpt-oss-120b | 20 | 5.6K | OSS GPT modelo |
| meta-llama/llama-4-scout-17b | 20 | 20K | Llama 4 Scout |
| whisper-large-v3-turbo | 14 | N/A | Audio (sin TPM) |

### PAID TIER (Developer Plan)

| Modelo | RPM | TPM | Mejora vs Free |
|--------|-----|-----|----------------|
| llama-3.3-70b-versatile | 800 | 240K | **40x RPM, 30x TPM** |
| llama-3.1-8b-instant | 800 | 200K | **40x RPM, 50x TPM** |
| whisper-large-v3-turbo | 320 | N/A | **23x RPM** |

---

## üéÆ COMANDOS

### Ver modelo actual y l√≠mites
```bash
> /model
```

### Cambiar modelo (FREE tier)
```bash
> /model llama-3.1-8b-instant
‚úÖ Model changed: llama-3.3-70b-versatile ‚Üí llama-3.1-8b-instant
üìä Rate limits updated for llama-3.1-8b-instant:
   TPM: 4,000 | RPM: 20
```

### Cambiar a modelo de PAGO (si tienes cuenta paid)
```python
# En el futuro se podr√≠a agregar:
> /model llama-3.3-70b-versatile --tier paid
```

---

## ‚ö†Ô∏è COMPORTAMIENTO ANTE L√çMITES

### Escenario 1: Requests R√°pidas Consecutivas

```bash
> lee archivo1.txt
> lee archivo2.txt
> lee archivo3.txt
> ... (20 requests en 1 minuto)
> lee archivo21.txt
‚†ã Processing... [Esperando ~30 segundos hasta el pr√≥ximo minuto]
```

**Por qu√©**: Alcanzaste el l√≠mite de 20 RPM (requests/minuto)

### Escenario 2: Request con Muchos Tokens

```bash
> analiza todo el proyecto en detalle
[El modelo usa ~6,000 tokens en su respuesta]
> analiza otro archivo grande
‚†ã Processing... [Esperando porque ya usaste ~6K tokens y el l√≠mite es 8K TPM]
```

**Por qu√©**: Alcanzaste ~75% del l√≠mite TPM (8,000 tokens/minuto)

### Escenario 3: Modelo con Alto TPM

```bash
> /model groq/compound
üìä Rate limits updated for groq/compound:
   TPM: 50,000 | RPM: 20

> [Puedes hacer requests mucho m√°s grandes sin esperar]
```

**Beneficio**: compound tiene **50K TPM** vs 8K del modelo default

---

## üîç DEBUGGING

### Ver uso actual de rate limiting

Agrega temporalmente en `main.py`:
```python
from .rate_limiter import get_rate_limiter

rate_limiter = get_rate_limiter()
usage = rate_limiter.get_current_usage()
print(f"DEBUG Rate Usage: {usage}")
```

Output:
```json
{
  "model": "llama-3.3-70b-versatile",
  "tier": "free",
  "tokens_this_minute": 2450,
  "requests_this_minute": 5,
  "effective_tpm": 8000,
  "effective_rpm": 20,
  "tpm_usage_percentage": 30.6,
  "rpm_usage_percentage": 25.0,
  "can_make_request": true
}
```

### Si el modelo se sigue cortando

**Opci√≥n 1**: Reducir m√°s los l√≠mites

Edita `model_limits.py`:
```python
"llama-3.3-70b-versatile": ModelLimits(
    rpm=15,      # Reducir de 20 a 15
    tpm=6_000,   # Reducir de 8K a 6K
    ...
)
```

**Opci√≥n 2**: Usar modelo con l√≠mites m√°s altos
```bash
> /model groq/compound  # 50K TPM vs 8K TPM
```

**Opci√≥n 3**: Upgrade a Paid tier
```bash
> /model llama-3.3-70b-versatile --tier paid
# 800 RPM, 240K TPM (vs 20 RPM, 8K TPM free)
```

---

## üìà OPTIMIZACIONES

### 1. Modelos R√°pidos para Tareas Simples

```bash
# Tarea simple: listar archivos
> /model llama-3.1-8b-instant
> lista los archivos del proyecto

# Tarea compleja: an√°lisis profundo
> /model llama-3.3-70b-versatile
> analiza la arquitectura del proyecto
```

### 2. Usar Compound para Alto Volumen

```bash
> /model groq/compound
# TPM: 50K (6x m√°s que default)
# Ideal para: procesar muchos archivos, an√°lisis masivo
```

### 3. Pausas Estrat√©gicas

Si haces muchas requests:
```bash
# Request 1-18: OK
# Request 19: Esperar 5 segundos
# Request 20: Esperar 10 segundos
# As√≠ evitas el l√≠mite de 20 RPM
```

---

## üéØ RECOMENDACIONES

### Para Uso FREE (Default)

1. **Usa llama-3.3-70b-versatile** para desarrollo normal
   - L√≠mites: 20 RPM, 8K TPM
   - Suficiente para la mayor√≠a de tareas

2. **Cambia a compound** si necesitas procesar mucho
   - L√≠mites: 20 RPM, **50K TPM**
   - Ideal para an√°lisis masivo

3. **Si tienes problemas de cortes**:
   - Reduce frecuencia de requests
   - O edita `model_limits.py` para ser m√°s conservador

### Para Uso PAID (Opcional)

1. **Upgrade a Developer Plan** si:
   - Necesitas >20 requests/minuto
   - Procesas proyectos grandes constantemente
   - No quieres esperas

2. **Beneficios**:
   - 40x m√°s RPM (800 vs 20)
   - 30x m√°s TPM (240K vs 8K)
   - Casi sin esperas

---

## ‚úÖ TESTING

### Test 1: Cambio de modelo
```bash
groq-custom
> /model llama-3.1-8b-instant
# Deber√≠a mostrar: "üìä Rate limits updated..."
```

### Test 2: Uso intensivo
```bash
> lee README.md
> lee main.py
> lee conversation.py
> ... (hacer 15-18 requests r√°pidas)
# No deber√≠a haber espera a√∫n

> ... (request 20-21)
# Deber√≠a mostrar: "‚†ã Processing..." y esperar
```

### Test 3: Modelo con alto TPM
```bash
> /model groq/compound
> analiza todo el proyecto [request grande, ~5K tokens]
# Deber√≠a funcionar sin espera (50K TPM de l√≠mite)
```

---

## üìù ARCHIVOS MODIFICADOS

1. **`model_limits.py`** (NUEVO)
   - Configuraci√≥n de l√≠mites por modelo
   - FREE y PAID tier
   - Muy conservador (70-80% de l√≠mites reales)

2. **`rate_limiter.py`** (ACTUALIZADO)
   - Tracking de RPM y TPM
   - Cambio din√°mico de modelo
   - M√©todo `update_model()` para `/model`

3. **`main.py`** (ACTUALIZADO)
   - `switch_model()` ahora actualiza rate limiter
   - Inicializaci√≥n del rate limiter con modelo default

---

## üéâ RESULTADO

Ahora tu Groq CLI:
- ‚úÖ **No se corta** porque usa l√≠mites conservadores
- ‚úÖ **Funciona bien** con modelos free
- ‚úÖ **Se adapta** autom√°ticamente al modelo
- ‚úÖ **Est√° listo** para upgrade a paid si lo necesitas

**Prioridad: Que funcione bien > Que sea r√°pido**
(Configuraci√≥n conservadora evita cortes y confusi√≥n del modelo)
